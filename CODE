#!/usr/bin/env python
# coding: utf-8

# In[ ]:

#import all possible libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt  
import seaborn as seabornInstance 
from sklearn.model_selection import train_test_split 
from sklearn.linear_model import LinearRegression
from sklearn import metrics
from sklearn.feature_selection import RFE 
import seaborn as sns 
from sklearn.linear_model import LinearRegression, Ridge, Lasso 
from sklearn.model_selection import train_test_split, cross_val_score 
from statistics import mean 
get_ipython().run_line_magic('matplotlib', 'inline')

# read the data
df = pd.read_csv('/Users/Jack/Desktop/Copy of IPEDS_data.csv')

# drop data with missing values in applicants total, since the lack of 'applicants total' data is meanless
df=df.dropna(subset=['Applicants total'])

# Fill in the remaining missing value with 0
df=df.fillna(0)

# Since some processing methods cannot handle with string, the csv file will drop the column that includes string
df = df.drop(["Name","Offers Less than one year certificate","Offers One but less than two years certificate","Offers Associate's degree",'Offers Two but less than 4 years certificate',"Offers Bachelor's degree",'Offers Postbaccalaureate certificate',"Offers Master's degree","Offers Post-master's certificate","Offers Doctor's degree - research/scholarship","Offers Doctor's degree - professional practice","Offers Doctor's degree - other","Offers Other degree"], axis=1)

# store new file after data cleaning 
df.to_csv('/Users/Jack/Desktop/Copy of IPEDS_data_1.csv')

#Multiple Linear Regression, this example adapted from Chauhan(2019)
dataset = pd.read_csv('/Users/Jack/Desktop/Copy of IPEDS_data_1.csv')
dataset.shape
dataset.describe()

# x axis is data from possible attributes, and y axis is data from 'applicants total', which is the target data need to predict
X = dataset[['Total  enrollment', 'Full-time enrollment', 'Part-time enrollment', 'Undergraduate enrollment', 'Graduate enrollment', 'Full-time undergraduate enrollment', 'Part-time undergraduate enrollment']]
y = dataset['Applicants total'].values

# draw a figure to determine the mean of applicats total
plt.figure(figsize=(15,10))
plt.tight_layout()
seabornInstance.distplot(dataset['Applicants total'])

# split 80% data to the training set, and 20% to the test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

#train model
regressor = LinearRegression()  
regressor.fit(X_train, y_train)

# present coefficient betwen columns in x axis ans y axis
coeff_df = pd.DataFrame(regressor.coef_, X.columns, columns=['Coefficient'])  
coeff_df



# Fit using the RFE model to do feature selection 
rfe = RFE(regressor, 7)
X_rfe = rfe.fit_transform(X_train, y_train)
regressor.fit(X_rfe, y_train)
print(rfe.ranking_)

nof_list=np.arange(1,12)            
high_score=0

nof=0           
score_list =[]
for n in range(len(nof_list)):
    rfe = RFE(regressor,nof_list[n])
    X_train_rfe = rfe.fit_transform(X_train, y_train)
    X_test_rfe = rfe.transform(X_test)
    regressor.fit(X_train_rfe, y_train)
    score = regressor.score(X_test_rfe, y_test)
    score_list.append(score)
    if(score>high_score):
        high_score = score
        nof = nof_list[n]
       
print("Optimum number of features: %d" %nof)
print("Score with %d features: %f" % (nof, high_score))
print(coeff_df)

#L1 and L2 regularization, this example adapted from AlindGupta in GeeksforGeeks.com
data = pd.read_csv('/Users/Jack/Desktop/Copy of IPEDS_data_1.csv')

#Separating the dependent and independent variables
y = data['Applicants total'] 
X = data.drop('Applicants total', axis = 1)

#divide data into testing set and training set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25) 

#Bulding the Linear Regression model 
linearModel = LinearRegression() 
linearModel.fit(X_train, y_train) 
print(linearModel.score(X_test, y_test))

#L2 regularization
cross_val_scores_ridge = [] 
alpha = [] 
#Loop to compute the different values of cross-validation scores
for i in range(1, 9): 
    ridgeModel = Ridge(alpha = i * 0.25) 
    ridgeModel.fit(X_train, y_train) 
    scores = cross_val_score(ridgeModel, X, y, cv = 10) 
    avg_cross_val_score = mean(scores)*100
    cross_val_scores_ridge.append(avg_cross_val_score) 
    alpha.append(i * 0.25) 

#Loop to print the different values of cross-validation scores
for i in range(0, len(alpha)): 
    print(str(alpha[i])+' : '+str(cross_val_scores_ridge[i])) 
    
 # L1 regularization 
ridgeModelChosen = Ridge(alpha = 2) 
ridgeModelChosen.fit(X_train, y_train) 
print(ridgeModelChosen.score(X_test, y_test))

cross_val_scores_lasso = []
Lambda = [] 
for i in range(1, 9): 
    lassoModel = Lasso(alpha = i * 0.25, tol = 0.0925) 
    lassoModel.fit(X_train, y_train) 
    scores = cross_val_score(lassoModel, X, y, cv = 10) 
    avg_cross_val_score = mean(scores)*100
    cross_val_scores_lasso.append(avg_cross_val_score) 
    Lambda.append(i * 0.25) 
for i in range(0, len(alpha)): 
    print(str(alpha[i])+' : '+str(cross_val_scores_lasso[i])) 


# after testing, the best value of lambda is 2    
lassoModelChosen = Lasso(alpha = 2, tol = 0.0925) 
lassoModelChosen.fit(X_train, y_train) 
print(lassoModelChosen.score(X_test, y_test)) 

#visulizing the results, and compare scores of different models 
models = ['Linear Regression', 'Ridge Regression', 'Lasso Regression'] 
scores = [linearModel.score(X_test, y_test), 
         ridgeModelChosen.score(X_test, y_test), 
         lassoModelChosen.score(X_test, y_test)] 
mapping = {} 
mapping['Linear Regreesion'] = linearModel.score(X_test, y_test) 
mapping['Ridge Regreesion'] = ridgeModelChosen.score(X_test, y_test) 
mapping['Lasso Regression'] = lassoModelChosen.score(X_test, y_test)
for key, val in mapping.items(): 
    print(str(key)+' : '+str(val)) 
plt.bar(models, scores) 
plt.xlabel('Regression Models') 
plt.ylabel('Score') 
plt.show() 

